{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ouA1idc---n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from scipy.sparse import csr_matrix\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial import distance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdQ5za_thkKo",
        "outputId": "4ba9c7ca-6a3e-4804-f5dd-4742212696e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfvbB6d65VG4"
      },
      "outputs": [],
      "source": [
        "def read_files():\n",
        "  docs=[]\n",
        "  docs_names=[]\n",
        "\n",
        "  path=\"/content/drive/MyDrive/FYP/Text_Files\"\n",
        "  dir_list = os.listdir(path)\n",
        "  #print(dir_list)\n",
        "\n",
        "  for f in dir_list:\n",
        "    with open(path+\"/\"+f, \"r\", encoding='Windows-1252') as file:\n",
        "      data = file.read()\n",
        "    data = data.replace(\"\\n\",\" \")\n",
        "    docs.append(data)\n",
        "    docs_names.append(f)\n",
        "\n",
        "  return docs, docs_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8xfZByvAtPS"
      },
      "outputs": [],
      "source": [
        "def tfidf_vectorization(docs):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  X  = vectorizer.fit_transform(docs)\n",
        "  return vectorizer, X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyLYPCPzG6In"
      },
      "outputs": [],
      "source": [
        "def load_dataframe():\n",
        "  df = pd.read_csv ('/content/drive/MyDrive/FYP/dataset.csv')\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwjICtebLVOm"
      },
      "outputs": [],
      "source": [
        "def encoding(encoder, Z):\n",
        "  compress = []\n",
        "  \n",
        "  for i in range(len(Z)) :\n",
        "    a = (encoder.predict(Z[i]).flatten())\n",
        "    compress.append(a)\n",
        "  \n",
        "  return compress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKf93Y8o-YoK"
      },
      "outputs": [],
      "source": [
        "def getQueryDoc(file):\n",
        "  with open(file, \"r\", encoding='Windows-1252') as file:\n",
        "    d = file.read()\n",
        "  d = d.replace(\"\\n\",\" \")\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(query):\n",
        "  docs, names = read_files()\n",
        "  vectorizer, tfidf = tfidf_vectorization(docs)\n",
        "\n",
        "  df = load_dataframe()\n",
        "  #print(df)\n",
        "\n",
        "  # reconstruct dense matrix\n",
        "  S = csr_matrix(tfidf)\n",
        "  Z = S.todense()\n",
        "  \n",
        "  model = load_model('/content/drive/MyDrive/FYP/saved_model/autoencoder.h5')\n",
        "\n",
        "  #extracting encoder part\n",
        "  encoder = Model(inputs=model.input, outputs=model.layers[-9].output)\n",
        "\n",
        "  compress = []\n",
        "  \n",
        "  for i in range(len(Z)) :\n",
        "    a = (encoder.predict(Z[i]).flatten())\n",
        "    compress.append(a)\n",
        "\n",
        "  doc1 = []\n",
        "  for i in range(len(df)) :\n",
        "    a = names.index(df.iloc[i,0])\n",
        "    doc1.append(compress[a])\n",
        "\n",
        "  df['doc1'] = doc1\n",
        "\n",
        "  doc2 = []\n",
        "  for i in range(len(df)) :\n",
        "    a = names.index(df.iloc[i,1])\n",
        "    doc2.append(compress[a])\n",
        "\n",
        "  df['doc2'] = doc2\n",
        "\n",
        "  #print(df)\n",
        "  \n",
        "  # Fitting K-Means to the dataset\n",
        "  kmeans = KMeans(n_clusters = 5, init = 'k-means++')\n",
        "  y_kmeans = kmeans.fit_predict(compress)\n",
        "\n",
        "  query_doc = getQueryDoc(query)\n",
        "\n",
        "  d = [query_doc]\n",
        "  dv = vectorizer.transform(d)  \n",
        "\n",
        "  q = csr_matrix(dv)\n",
        "  # reconstruct dense matrix\n",
        "  qq = q.todense()\n",
        "\n",
        "  query = encoder.predict(qq)\n",
        "  query_doc = query.reshape(3930,)\n",
        "\n",
        "  centroid = kmeans.cluster_centers_\n",
        "\n",
        "  euc_dis = []\n",
        "  for i in centroid:\n",
        "    dis = distance.euclidean(query_doc, i)\n",
        "    euc_dis.append(dis)\n",
        "\n",
        "  k = euc_dis.index(max(euc_dis))\n",
        "  \n",
        "  query_doc = query_doc.reshape(1, 3930)\n",
        "\n",
        "  # DataFrame \n",
        "  df1 = pd.DataFrame(names, columns =['Document_Name'])  \n",
        "  df1['Document']= docs\n",
        "\n",
        "  siamese_model = load_model('/content/drive/MyDrive/FYP/saved_model/siamesemodel.h5')\n",
        "  #print(siamese_model.summary())\n",
        "\n",
        "  for i in range(len(compress)):\n",
        "    compress[i]= compress[i].reshape(1,3930)\n",
        "\n",
        "  df1['Cluster_id']=y_kmeans\n",
        "  df1['Embedding']=compress\n",
        "  #print(df1)\n",
        "\n",
        "  similarity_index = []\n",
        "  source_doc_name=[]\n",
        "  source_doc=[]\n",
        "  for i in range(len(df1)):\n",
        "    if(df1['Cluster_id'][i])==k:\n",
        "      similarity_index.append(siamese_model.predict([query_doc, df1['Embedding'][i]]))\n",
        "      source_doc_name.append(df1['Document_Name'][i])\n",
        "      source_doc.append(df1['Document'][i])\n",
        "\n",
        "  similarity_index = np.array(similarity_index)\n",
        "  similarity_index = similarity_index.flatten()\n",
        "\n",
        "  max_ind = np.argsort(similarity_index)[::-1][:10]\n",
        "  Document_Names=[source_doc_name[i] for i in max_ind]\n",
        "  Document= [source_doc[i] for i in max_ind]\n",
        "  Similarity_Index= similarity_index[max_ind]\n",
        "\n",
        "  df2 = pd.DataFrame(Document_Names, columns =['Document_Name'])  \n",
        "  df2['Data'] = Document\n",
        "  df2['Similarity_Index'] = Similarity_Index\n",
        "  \n",
        "  print(df2)\n",
        "  return Document_Names, Document, Similarity_Index"
      ],
      "metadata": {
        "id": "x391mjTjCTqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PHpjd_3MCsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec0eb39-f932-41dc-dd5e-fc04b06e5efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Document_Name                                               Data  \\\n",
            "0  D13-1055.pdf.txt  Proceedings of the 2013 Conference on Empirica...   \n",
            "1  D13-1084.pdf.txt  Proceedings of the 2013 Conference on Empirica...   \n",
            "2  D12-1066.pdf.txt  Proceedings of the 2012 Joint Conference on Em...   \n",
            "3  D13-1041.pdf.txt  Proceedings of the 2013 Conference on Empirica...   \n",
            "4  D12-1082.pdf.txt  Proceedings of the 2012 Joint Conference on Em...   \n",
            "5  D11-1141.pdf.txt  Proceedings of the 2011 Conference on Empirica...   \n",
            "6  D11-1092.pdf.txt  Proceedings of the 2011 Conference on Empirica...   \n",
            "7  D09-1130.pdf.txt  Proceedings of the 2009 Conference on Empirica...   \n",
            "8  D08-1072.pdf.txt  Proceedings of the 2008 Conference on Empirica...   \n",
            "9  A88-1000.pdf.txt  Second  Conference   on  Applied  Natural Lang...   \n",
            "\n",
            "   Similarity_Index  \n",
            "0          0.293023  \n",
            "1          0.292493  \n",
            "2          0.280775  \n",
            "3          0.279848  \n",
            "4          0.279286  \n",
            "5          0.274969  \n",
            "6          0.274137  \n",
            "7          0.268731  \n",
            "8          0.259626  \n",
            "9          0.255034  \n"
          ]
        }
      ],
      "source": [
        "Document_Names, Document, Similarity_Index = main('/content/drive/MyDrive/Dataset/D13-1047.pdf.txt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}