{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LCS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PoFQbxrb8nA",
        "outputId": "dccba28a-184e-41c5-c8bb-5fbf107a924a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Score Matrix\n",
        "score_mat={}"
      ],
      "metadata": {
        "id": "9rB8lmgsOgSE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZIZu3gwc3J-",
        "outputId": "f8e9c231-fac3-4edc-9264-2c9b24673a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(query, f, doc):\n",
        "  #word tokenization\n",
        "  tokens_o=word_tokenize(query)\n",
        "  tokens_p=word_tokenize(doc)\n",
        "\n",
        "  #lowerCase\n",
        "  tokens_o = [token.lower() for token in tokens_o]\n",
        "  tokens_p = [token.lower() for token in tokens_p]\n",
        "  #print(tokens_o)\n",
        "  #print(tokens_p)\n",
        "\n",
        "  #stop word removal\n",
        "  #punctuation removal\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  punctuations=['\"','.','(',')',',','?',';',':',\"''\",'``']\n",
        "  filtered_tokens_o = [w for w in tokens_o if not w in stop_words and not w in punctuations]\n",
        "  filtered_tokens_p = [w for w in tokens_p if not w in stop_words and not w in punctuations]\n",
        " \n",
        "  #longest common subsequence\n",
        "  #dynamic programming algorithm for finding lcs\n",
        "  def lcs(l1,l2):\n",
        "    s1=word_tokenize(l1)\n",
        "    s2=word_tokenize(l2)\n",
        "    # storing the dp values \n",
        "    dp = [[None]*(len(s1)+1) for i in range(len(s2)+1)] \n",
        "  \n",
        "    for i in range(len(s2)+1): \n",
        "        for j in range(len(s1)+1): \n",
        "            if i == 0 or j == 0: \n",
        "                dp[i][j] = 0\n",
        "            elif s2[i-1] == s1[j-1]: \n",
        "                dp[i][j] = dp[i-1][j-1]+1\n",
        "            else: \n",
        "                dp[i][j] = max(dp[i-1][j] , dp[i][j-1]) \n",
        "    return dp[len(s2)][len(s1)] \n",
        "\n",
        "  sent_o=sent_tokenize(query)\n",
        "  sent_p=sent_tokenize(doc)\n",
        "\n",
        "  #maximum length of LCS for a sentence in suspicious text\n",
        "  max_lcs=0\n",
        "  sum_lcs=0\n",
        "\n",
        "  for i in sent_p:\n",
        "    for j in sent_o:\n",
        "        l=lcs(i,j)\n",
        "        max_lcs=max(max_lcs,l)\n",
        "    sum_lcs+=max_lcs\n",
        "    max_lcs=0\n",
        "\n",
        "  score=sum_lcs/len(tokens_p)\n",
        "  score_mat[f]=score\n",
        "  #return score_mat\n"
      ],
      "metadata": {
        "id": "1PZaX59iRBVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/TextFiles2/D13-1081.pdf.txt', \"r\", encoding='cp1252') as f:\n",
        "    sequence_1 = f.read()\n",
        "    #print(sequence_1)\n",
        "sequence_1 = sequence_1.replace(\"\\n\",\"\")\n",
        "query_doc = sequence_1.replace(\" \",\"\")    "
      ],
      "metadata": {
        "id": "y8yQy4AiOwN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        " \n",
        "#Dictionary\n",
        "dict={}\n",
        "\n",
        "path=\"/content/drive/MyDrive/TextFiles\"\n",
        "dir_list = os.listdir(path)\n",
        "\n",
        "for f in dir_list:\n",
        "  with open(path+\"/\"+f, \"r\", encoding='cp1252') as file:\n",
        "    data = file.read()\n",
        "  data = data.replace(\"\\n\",\"\")\n",
        "  data = data.replace(\" \",\"\")    \n",
        "  dict[f]=data;\n",
        "  #print(f)\n",
        "\n",
        "#print(dict['D13-1080.pdf.txt'])\n"
      ],
      "metadata": {
        "id": "u_QbMCFyJpIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in dict:\n",
        "  preprocess(query_doc, key, dict[key])\n",
        "print(score_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaRdjEs0mp1M",
        "outputId": "b068c723-047d-46a1-964c-1b7a87439cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'D13-1080.pdf.txt': 0.34201077199281865, 'D13-1081.pdf.txt': 1.0, 'D13-1082.pdf.txt': 0.34947768281101615, 'D13-1083.pdf.txt': 0.3062730627306273, 'D13-1084.pdf.txt': 0.3590568060021436, 'D13-1079.pdf.txt': 0.3275862068965517, 'D13-1078.pdf.txt': 0.16006256517205422, 'D13-1077.pdf.txt': 0.21546052631578946, 'D13-1076.pdf.txt': 0.39731543624161075, 'D13-1075.pdf.txt': 0.4397590361445783, 'D13-1074.pdf.txt': 0.2434325744308231, 'D13-1073.pdf.txt': 0.3153423288355822, 'D13-1072.pdf.txt': 0.3794618341570566, 'D13-1071.pdf.txt': 0.13712938005390835, 'D13-1070.pdf.txt': 0.2742032471437162, 'D13-1069.pdf.txt': 0.2699896157840083, 'D13-1068.pdf.txt': 0.22346996069623806, 'D13-1067.pdf.txt': 0.26691042047531993, 'D13-1066.pdf.txt': 0.20462232243517475, 'D13-1065.pdf.txt': 0.18361344537815127, 'D13-1064.pdf.txt': 0.2723242558581381, 'D13-1063.pdf.txt': 0.21597187343043697, 'D13-1062.pdf.txt': 0.20076849183477424, 'D13-1061.pdf.txt': 0.23968947113051917, 'D13-1060.pdf.txt': 0.20889555222388806, 'D13-1059.pdf.txt': 0.12297426120114395, 'D13-1058.pdf.txt': 0.24723247232472326, 'D13-1057.pdf.txt': 0.16539235412474848, 'D13-1056.pdf.txt': 0.2180232558139535, 'D13-1055.pdf.txt': 0.255663430420712, 'D13-1054.pdf.txt': 0.22394173873463813, 'D13-1053.pdf.txt': 0.3438874230430959, 'D13-1052.pdf.txt': 0.2125556104794859, 'D13-1051.pdf.txt': 0.2440264361972547, 'D13-1050.pdf.txt': 0.2637037037037037, 'D13-1049.pdf.txt': 0.2589974293059126, 'D13-1048.pdf.txt': 0.16106531388712747, 'D13-1047.pdf.txt': 0.2892988929889299, 'D13-1046.pdf.txt': 0.29891304347826086, 'D13-1045.pdf.txt': 0.18204911092294665}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_key = max(score_mat, key=score_mat.get)\n",
        "print(\"Most similar document: '\"+ max_key+ \"' with similarity score: \"+ str(score_mat[max_key]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PADSRKmms-GV",
        "outputId": "39dffe37-d097-404d-970b-1ae37314b390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar document: 'D13-1081.pdf.txt' with similarity score: 1.0\n"
          ]
        }
      ]
    }
  ]
}