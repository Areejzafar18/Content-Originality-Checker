# -*- coding: utf-8 -*-
"""Copy of Copy of Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tr0VdiIXsVv_BjkKtrGKE_Oc9wTJhbBX
"""

from fileinput import filename
import numpy as np
import pandas as pd
import tensorflow as tf
import os
from scipy.sparse import csr_matrix
from keras.models import load_model
from tensorflow.keras.models import Model
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from scipy.spatial import distance
import uuid

def read_files():
  docs=[]
  docs_names=[]

  path="Text_Files"
  dir_list = os.listdir(path)
  #print(dir_list)

  for f in dir_list:
    with open(path+"/"+f, "r", encoding='Windows-1252') as file:
      data = file.read()
    data = data.replace("\n"," ")
    docs.append(data)
    docs_names.append(f)

  return docs, docs_names

def tfidf_vectorization(docs):
  vectorizer = TfidfVectorizer()
  X  = vectorizer.fit_transform(docs)
  return vectorizer, X

def load_dataframe():
  df = pd.read_csv ('dataset.csv')
  return df

def encoding(encoder, Z):
  compress = []
  
  for i in range(len(Z)) :
    a = (encoder.predict(Z[i]).flatten())
    compress.append(a)
  
  return compress

def getQueryDoc(file):
  with open(file, "r", encoding='Windows-1252') as file:
    d = file.read()
  d = d.replace("\n"," ")
  return d

def get_clusters_index(y_kmeans):
  k0 = []
  k1 = []
  k2 = []
  k3 = []
  k4 = []

  for i in range(len(y_kmeans)):
    if y_kmeans[i]==0:
      k0.append(i)
    if y_kmeans[i]==1:
      k1.append(i)
    if y_kmeans[i]==2:
      k2.append(i)
    if y_kmeans[i]==3:
      k3.append(i)
    if y_kmeans[i]==4:
      k4.append(i)

  return k0, k1 ,k2 ,k3 ,k4

def get_clusters( k0, k1 ,k2 ,k3 ,k4, compress):
  K0 = []
  K1 = []
  K2 = []
  K3 = []
  K4 = []

  for i in range(len(k0)):
    K0.append(compress[i])

  for i in range(len(k1)):
    K1.append(compress[i])

  for i in range(len(k2)):
    K2.append(compress[i])

  for i in range(len(k3)):
    K3.append(compress[i])

  for i in range(len(k4)):
    K4.append(compress[i])

  return K0, K1 ,K2 ,K3 ,K4

def main(query):
  docs, names = read_files()
  vectorizer, tfidf = tfidf_vectorization(docs)

  df = load_dataframe()
  #print(df)

  # reconstruct dense matrix
  S = csr_matrix(tfidf)
  Z = S.todense()
  
  model = load_model('autoencoder.h5')

  #extracting encoder part
  encoder = Model(inputs=model.input, outputs=model.layers[-9].output)

  compress = []
  
  for i in range(len(Z)) :
    a = (encoder.predict(Z[i]).flatten())
    compress.append(a)

  doc1 = []
  for i in range(len(df)) :
    a = names.index(df.iloc[i,0])
    doc1.append(compress[a])

  df['doc1'] = doc1

  doc2 = []
  for i in range(len(df)) :
    a = names.index(df.iloc[i,1])
    doc2.append(compress[a])

  df['doc2'] = doc2

  #print(df)
  
  # Fitting K-Means to the dataset
  kmeans = KMeans(n_clusters = 5, init = 'k-means++')
  y_kmeans = kmeans.fit_predict(compress)

  query_doc = getQueryDoc(query)

  d = [query_doc]
  dv = vectorizer.transform(d)  

  q = csr_matrix(dv)
  # reconstruct dense matrix
  qq = q.todense()

  query = encoder.predict(qq)
  query_doc = query.reshape(3930,)

  centroid = kmeans.cluster_centers_

  euc_dis = []
  for i in centroid:
    dis = distance.euclidean(query_doc, i)
    euc_dis.append(dis)

  k = euc_dis.index(max(euc_dis))
  
  query_doc = query_doc.reshape(1, 3930)

  # DataFrame 
  df1 = pd.DataFrame(names, columns =['Document_Name'])  
  df1['Document']= docs

  siamese_model = load_model('siamesemodel.h5')
  #print(siamese_model.summary())

  for i in range(len(compress)):
    compress[i]= compress[i].reshape(1,3930)

  df1['Cluster_id']=y_kmeans
  df1['Embedding']=compress
  #print(df1)

  similarity_index = []
  source_doc_name=[]
  source_doc=[]
  for i in range(len(df1)):
    if(df1['Cluster_id'][i])==k:
      similarity_index.append(siamese_model.predict([query_doc, df1['Embedding'][i]]))
      source_doc_name.append(df1['Document_Name'][i])
      source_doc.append(df1['Document'][i])

  similarity_index = np.array(similarity_index)
  similarity_index = similarity_index.flatten()

  max_ind = np.argsort(similarity_index)[::-1][:10]
  Document_Names=[source_doc_name[i] for i in max_ind]
  Document= [source_doc[i] for i in max_ind]
  Similarity_Index= similarity_index[max_ind]
  return Document_Names, Document, Similarity_Index


def add_file_to_temporary(file):
  ext = file.filename.split('.')[-1]
  uname = str(uuid.uuid4())
  fname = os.path.join("TemporaryFolder", f'{uname}.{ext}')
  file.save( fname )
  return fname

def remove_file_from_temporary_folder(fname):
  try:
    os.remove(fname)
    return True
  except:
    return False

def solve(fname):
  Document_Names, Document, Similarity_Index = main(fname)
  data = [[name, str(index),] for name, index in zip(Document_Names, Similarity_Index) ]
  data.sort(key=lambda i:i[1],reverse=True)
  return data

# Similarity_docs=Document_Names, Document, Similarity_Index = main('/content/drive/MyDrive/Dataset/D13-1047.pdf.txt')
# Similarity_docs
