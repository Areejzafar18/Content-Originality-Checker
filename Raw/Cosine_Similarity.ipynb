{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cosine_Similarity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp1BNegrUx6J"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTKceQKrVQmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b228ce-16b9-413d-83fc-898ed326d94b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "FZ1S6glMA2KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        " \n",
        "#Dictionary\n",
        "docs={}\n",
        "\n",
        "path=\"/content/drive/MyDrive/fyp/TextFiles\"\n",
        "dir_list = os.listdir(path)\n",
        "\n",
        "for f in dir_list:\n",
        "  with open(path+\"/\"+f, \"r\", encoding='cp1252') as file:\n",
        "    data = file.read()\n",
        "  data = data.replace(\"\\n\",\" \")\n",
        "  docs[f]=data;\n",
        "  #print(f)"
      ],
      "metadata": {
        "id": "1pIJRY8tA8l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "    punctuations = '''!()–-[]{};:'\"\\,<>./?@#$%^&*_~+’|ˆ'''\n",
        "    for key in data:\n",
        "      no_punct = \"\"\n",
        "      str1 = data[key]\n",
        "      for char in str1:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "      \n",
        "      data[key]=no_punct.lower()\n",
        "    return data"
      ],
      "metadata": {
        "id": "FmufFqIVBJ5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(docs)"
      ],
      "metadata": {
        "id": "gQ1ZRqFRBLdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yci7QjktVPtt",
        "outputId": "16c5726e-d685-47e3-8a1b-034a9068bd1b"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(docs.values())\n",
        "\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40, 19833)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk3jD9SjvDxU"
      },
      "source": [
        "query_path = '/content/drive/MyDrive/fyp/TextFiles/D13-1082.pdf.txt'\n",
        "q = open(\"/content/drive/MyDrive/fyp/TextFiles/D13-1082.pdf.txt\", \"r\", encoding='unicode_escape')\n",
        "query_doc = q.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(data):\n",
        "    punctuations = '''!()–-[]{};:'\"\\,<>./?@#$%^&*_~+’|ˆ'''\n",
        "    for char in data:\n",
        "      if char not in punctuations:\n",
        "        no_punct = no_punct + char\n",
        "        \n",
        "    data=no_punct.lower()\n",
        "    return data"
      ],
      "metadata": {
        "id": "pFsZ8-Q7aKCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_doc=process(query_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "0RJV9il7aXAv",
        "outputId": "018d3b11-7196-44f9-f7f5-25927ea59f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-9663274ecf33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery_doc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-34f764cf6a5a>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mno_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_punct\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_punct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'no_punct' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import re\n",
        "re.compile('<title>(.*)</title>')\n",
        "text=re.sub(r'[^\\w ]+', \"\",query_doc)\n",
        "\n",
        "text = re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", text)\n",
        "text=text.lower()\n",
        "\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "stw= remove_stopwords(text)\n",
        "\n",
        "#Lemmatization\n",
        "#converts to base form like feet to foot\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "stw = nltk.word_tokenize(stw);\n",
        "\n",
        "lemmatized_text=[]\n",
        "for words in stw:\n",
        "        lemmatized_text.append(lemmatizer.lemmatize(words))\n",
        "\n",
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "  \n",
        "ps = PorterStemmer()\n",
        "stem_text=[]\n",
        "for words in lemmatized_text:\n",
        "        stem_text.append(ps.stem(words))\n",
        "print(stem_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufXm2pSfALBI",
        "outputId": "1e57152c-6ce9-4d42-f84f-6b4bc709ec42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "['proceed', 'confer', 'empir', 'method', 'natur', 'languag', 'process', 'page', '845850seattl', 'washington', 'usa', 'octob', 'c2013', 'associ', 'comput', 'linguisticsconvert', 'continuousspac', 'languag', 'model', 'intongram', 'languag', 'model', 'statist', 'machin', 'translationrui', 'wang123', 'masao', 'utiyama2', 'isao', 'goto2', 'eiichro', 'sumita2', 'hai', 'zhao13', 'baoliang', 'lu131', 'center', 'brainlik', 'comput', 'machin', 'intelligencedepart', 'scienc', 'engineeringshanghai', 'jiao', 'tong', 'unviers', 'shanghai', 'china2', 'multilingu', 'translat', 'laboratori', 'mastar', 'projectn', 'institut', 'inform', 'commun', 'technology35', 'hikaridai', 'keihanna', 'scienc', 'citi', 'kyoto', 'japan3', 'moemicrosoft', 'key', 'lab', 'intellig', 'comput', 'intellig', 'systemsshanghai', 'jiao', 'tong', 'unviers', 'shanghai', 'chinawangruinlpgmailcom', 'mutiyamaigotoeiichirosumitanictgojp', 'zhaohaicssjtueducn', 'bllusjtueducnabstractneur', 'network', 'languag', 'model', 'orcontinuousspac', 'languag', 'model', 'cslmshave', 'shown', 'improv', 'performanceof', 'statist', 'machin', 'translat', 'smtwhen', 'rerank', 'nbesttransl', 'cslm', 'notbeen', 'pa', 'decod', 'smtbecaus', 'cslm', 'decod', 'take', 'lotof', 'time', 'contrast', 'propos', 'methodfor', 'convert', 'cslm', 'backoff', 'ngramlanguag', 'model', 'bnlm', 'canus', 'convert', 'cslm', 'decod', 'showthat', 'outperform', 'origin', 'bnlm', 'andar', 'compar', 'tradit', 'use', 'ofcslm', 'reranking1', 'introductionlanguag', 'model', 'import', 'natur', 'languageprocess', 'task', 'speech', 'recognit', 'andstatist', 'machin', 'translat', 'tradit', 'backoff', 'ngram', 'languag', 'model', 'bnlm', 'chen', 'andgoodman', 'chen', 'goodman', '1998stolck', 'wide', 'tasksrec', 'neural', 'network', 'languag', 'modelsor', 'continuousspac', 'languag', 'model', 'cslmsbengio', 'et', 'al', 'schwenk', 'le', 'et', 'al', '2011are', 'statist', 'machin', 'translationsmt', 'schwenk', 'et', 'al', 'son', 'et', 'al', '2010schwenk', 'et', 'al', 'son', 'et', 'al', 'niehuesand', 'waibel', 'work', 'shown', 'thatcslm', 'improv', 'bleu', 'papineni', 'et', 'al2002', 'score', 'smt', 'compar', 'bnlmson', 'condit', 'train', 'data', 'languagemodel', 'size', 'practicecslm', 'wide', 'smtone', 'reason', 'comput', 'cost', 'oftrain', 'cslm', 'high', 'variousmethod', 'propos', 'tackl', 'trainingcost', 'issu', 'son', 'et', 'al', 'schwenk', 'et', 'al', '2012mikolov', 'et', 'al', 'littlework', 'reduc', 'cost', 'costsof', 'cslm', 'high', 'difficult', 'use', 'cslmsin', 'decod', 'directlya', 'common', 'approach', 'smt', 'cslm', 'isth', 'pa', 'approach', 'nbest', 'rerank', 'thisapproach', 'pa', 'us', 'bnlm', 'decodingto', 'produc', 'nbest', 'list', 'cslm', 'torerank', 'nbest', 'translat', 'second', 'passschwenk', 'et', 'al', 'son', 'et', 'al', 'schwenk', 'etal', 'son', 'et', 'al', '2012anoth', 'approach', 'restrict', 'boltzmannmachin', 'rbm', 'niehu', 'waibel', '2012instead', 'multilay', 'neural', 'networksbengio', 'et', 'al', 'schwenk', 'le', 'et', 'al2011', 'probabl', 'rbm', 'calculatedveri', 'effici', 'niehu', 'waibel', 'theycan', 'use', 'rbm', 'languag', 'model', 'smt', 'decodinghowev', 'rbm', 'adapt', 'ofsmt', 'larg', 'smt', 'task', 'trainingcost', 'rbm', 'highth', 'approach', 'bnlm', 'simulatea', 'cslm', 'deora', 'et', 'al', 'arsoy', 'et', 'al', '2013deora', 'et', 'al', 'recurr', 'neural', 'networklanguag', 'model', 'rnnlm', 'gener', 'largeamount', 'text', 'gener', 'samplingword', 'probabl', 'distribut', 'calculatedbi', 'rnnlm', 'train', 'bnlm845from', 'text', 'interpol', 'kneserneysmooth', 'method', 'arsoy', 'et', 'al', 'convertedneur', 'network', 'languag', 'model', 'increas', 'orderto', 'prune', 'backoff', 'languag', 'model', 'lowerord', 'model', 'constrain', 'ngram', 'allow', 'inhigherord', 'modelsboth', 'method', 'decod', 'forspeech', 'recognit', 'method', 'appliedto', 'notsolarg', 'scale', 'experi', 'million', 'mword', 'train', 'bnlm', 'arsoy', 'et', 'al2013', 'contrast', 'method', 'appli', 'smtand', 'improv', 'bnlm', 'creat', 'from746', 'm', 'word', 'cslm', 'train', 'mwordsbecaus', 'bnlm', 'train', 'largercorpora', 'trainingcslm', 'improv', 'bnlm', 'cslmtrain', 'smaller', 'corpu', 'importantactu', 'cslm', 'train', 'smaller', 'corpuscan', 'improv', 'bleu', 'score', 'smt', 'usedin', 'nbest', 'rerank', 'schwenk', 'huang', 'etal', 'contrast', 'demonstr', 'abnlm', 'simul', 'cslm', 'improv', 'bleuscor', 'smt', 'pa', 'decodingour', 'approach', 'follow', 'train', 'acslm', 'schwenk', 'corpu', 'secondw', 'train', 'bnlm', 'corpu', 'orlarg', 'corpu', 'final', 'rewrit', 'probabilityof', 'ngram', 'bnlm', 'probabilitycalcul', 'cslmwe', 'renorm', 'theprob', 'bnlm', 'use', 'rewrittenbnlm', 'smt', 'decodingin', 'section', 'bnlm', 'cslmschwenk', 'rewrit', 'bnlm', 'insect', 'method', 'convertinga', 'cslm', 'bnlm', 'section', 'weevalu', 'method', 'conclude2', 'languag', 'modelsin', 'section', 'introduc', 'standardbnlm', 'cslm', 'structur', 'probabilitycalculation21', 'standard', 'backoff', 'ngram', 'languag', 'modela', 'bnlm', 'predict', 'probabl', 'wordwi', 'givenit', 'preced', 'n', 'word', 'hi', 'wi1in1', 'butit', 'suffer', 'data', 'spars', 'contexthi', 'appear', 'train', 'data', 'anestim', 'backingoff', 'model', 'smallerhistori', 'necessari', 'case', 'modifiedkneserney', 'smooth', 'chen', 'goodman', '1998the', 'probabl', 'wi', 'given', 'hi', 'bnlmpbwihi', 'ispbwihi', 'pbwihi', 'hipbwiwi1in2', '1where', 'pbwihi', 'discount', 'probabl', 'andhi', 'backoff', 'weight', 'bnlm', 'witha', 'cslm', 'shown', 'below22', 'cslm', 'structur', 'probabilitycalculationth', 'main', 'structur', 'cslm', 'multilay', 'neural', 'network', 'contain', 'layer', 'inputlay', 'project', 'word', 'context', 'hi', 'ontoth', 'project', 'layer', 'hidden', 'layer', 'thesecond', 'hidden', 'layer', 'output', 'layer', 'achiev', 'thenonlin', 'probabl', 'estim', 'calcul', 'thelanguag', 'model', 'probabl', 'p', 'wihi', 'givencontext', 'schwenk', '2007the', 'cslm', 'calcul', 'probabl', 'allword', 'vocabulari', 'corpu', 'giventh', 'context', 'thecomput', 'complex', 'calcul', 'theprob', 'word', 'high', 'cslm', 'isonli', 'calcul', 'probabl', 'subsetof', 'vocabulari', 'subset', 'calleda', 'shortlist', 'consist', 'frequentword', 'vocabulari', 'cslm', 'calculatesth', 'sum', 'probabl', 'word', 'theshortlist', 'assign', 'neuron', 'purposeth', 'probabl', 'word', 'shortlistar', 'obtain', 'bnlm', 'schwenk', '2007schwenk', '2010let', 'wi', 'hi', 'current', 'word', 'histori', 'thecslm', 'bnlm', 'calcul', 'probabl', 'ofwi', 'given', 'hi', 'p', 'wihi', 'followsp', 'wihi', 'pcwihi1pcohipshi', 'wi', 'shortlistpbwihi', 'otherwise2wher', 'pc', 'probabl', 'calcul', 'thecslm', 'pcohi', 'probabl', 'neuronfor', 'word', 'shortlist', 'pb', 'theprob', 'calcul', 'bnlm', 'eq', '1andpshi', 'vshortlistpbvhi', '3846it', 'consid', 'cslm', 'redistributesth', 'probabl', 'mass', 'word', 'shortlistthi', 'probabl', 'mass', 'calcul', 'thebnlm3', 'convers', 'cslm', 'bnlma', 'describ', 'introduct', 'train', 'acslm', 'corpu', 'train', 'bnlm', 'fromth', 'corpu', 'larger', 'corpu', 'rewriteth', 'probabl', 'ngram', 'bnlm', 'theprob', 'calcul', 'cslmfirst', 'use', 'probabl', '1gram', 'inth', 'bnlm', 'rewrit', 'theprob', 'ngram', 'n2345', 'bnlmwith', 'probabl', 'calcul', 'ngramcslm', 'respect', 'note', 'ngram', 'cslmmean', 'length', 'histori', 'n', 'notealso', 'need', 'rewrit', 'probabilitiesof', 'ngram', 'end', 'word', 'shortlistfin', 'renorm', 'probabl', 'thebnlm', 'srilm', 'renorm', 'optionwhen', 'rewrit', 'bnlm', 'train', 'largercorpu', 'ngram', 'bnlm', 'containunknown', 'word', 'cslm', 'case', 'useth', 'probabl', 'bnlm', 'are4', 'experiments41', 'common', 'settingsw', 'patent', 'data', 'chines', 'englishpat', 'translat', 'subtask', 'ntcir9', 'patenttransl', 'task', 'goto', 'et', 'al', 'paralleltrain', 'develop', 'test', 'data', 'consist', '1m', 'sentenc', 'respectivelyw', 'follow', 'set', 'ntcir9', 'chineseto', 'english', 'translat', 'baselin', 'goto', 'et', 'al2011', 'languag', 'modelsto', 'compar', 'mose', 'phrasebas', 'smt', 'koehn', 'et', 'al', 'togetherwith', 'giza', 'och', 'ney', 'align', 'andmert', 'och', 'tune', 'developmentdata', 'translat', 'perform', 'measur', 'byth', 'caseinsensit', 'bleu', 'score', 'tokenizedtest', 'data', 'mtevalv13apl', 'forcalcul', 'bleu', 'scores11it', 'avail', 'httpwwwitlnistgoviadmigtestsmt2009w', 'standard', 'smt', 'featur', 'fivetransl', 'model', 'score', 'word', 'penalti', 'scoreseven', 'distort', 'score', 'languag', 'modelscor', 'differ', 'languag', 'model', 'wasus', 'calcul', 'languag', 'model', 'scorea', 'baselin', 'bnlm', 'train', '5grambnlm', 'modifi', 'kneserney', 'smooth', 'usingth', 'english', 'm', 'sentenc', 'train', 'datawhich', 'consist', 'm', 'word', 'discardani', 'ngram', 'train', 'model', 'wedid', 'use', 'count', 'cutoff', 'bnlm', 'asbnlm42a', '5gram', 'cslm', 'train', 'same1', 'm', 'train', 'sentenc', 'cslm', 'toolkitschwenk', 'set', 'cslmwere', 'project', 'layer', 'dimens', 'eachword', 'hidden', 'layer', 'dimens', 'outputlay', 'shortlist', 'dimens', 'wererecommend', 'cslm', 'toolkit', 'thiscslm', 'cslm42', 'cslm42', 'bnlm42', 'thebackground', 'bnlmwe', 'train', 'larger', '5gram', 'bnlm', 'withmodifi', 'kneserney', 'smooth', 'addingsent', 'patent', 'data', 'distributedin', 'ntcir8', 'patent', 'translat', 'task', 'fujii', 'et', 'al2010', 'm', 'word', 'data', 'consist', 'of746', 'm', 'word', 'bnlm', 'bnlm746', 'wediscard', '345gram', 'occur', 'whenw', 'creat', 'bnlm746next', 'rewrot', 'bnlm42', 'cslm42', 'byus', 'method', 'describ', 'section', 'thisrewritten', 'bnlm', 'interpol', 'bnlm42the', 'interpol', 'weight', 'determin', 'gridsearch', 'chang', 'interpol', 'weightto', 'creat', 'interpolatedbnlm', 'bnlm', 'smtsystem', 'tune', 'weight', 'paramet', 'firsthalf', 'develop', 'data', 'selectedth', 'interpol', 'weight', 'obtain', 'highestbleu', 'score', 'second', 'half', 'developmentdata', 'select', 'interpol', 'weightw', 'appli', 'mert', 'sentencedevelop', 'data', 'tune', 'weight', 'parameters2w', 'bnlm', 'conv42', 'obtainedconv746', 'rewrit', 'bnlm746', 'cslm422we', 'awar', 'interpol', 'weight', 'bedetermin', 'minim', 'perplex', 'developmentdata', 'opt', 'directli', 'maxim', 'bleu', 'score847in', 'wayth', 'vocabulari', 'languag', 'model', 'thesam', 'extract', 'm', 'trainingsentences42', 'experiment', 'resultst', 'show', 'percent', 'bleu', 'score', 'testdata', 'figur', '1st', 'pa', 'column', 'showth', 'bleu', 'score', 'pa', 'decod', 'whenw', 'chang', 'languag', 'model', 'figur', 'thererank', 'column', 'bleu', 'score', 'whenw', 'appli', 'cslm42', 'rerank', '100best', 'list', 'forth', 'differ', 'languag', 'model', 'appliedcslm42', 'rerank', 'ad', 'cslm42score', 'addit', '15th', 'featur', 'weightparamet', 'tune', 'zmert', 'zaidan2009lm', '1st', 'pa', 'rerankbnlm42', '3244conv42', '3298bnlm746', '3336conv746', '3354tabl', 'comparison', 'bleu', 'scoresw', 'perform', 'pair', 'bootstrap', 'resampl', 'test', 'koehn', 'sampl', '2000sampl', 'signific', 'testtabl', 'show', 'result', 'statisticalsignific', 'test', '1st', 'short', 'forth', '1st', 'pa', 'mark', 'indic', 'thelm', 'left', 'mark', 'significantli', 'betterthan', 'mark', 'certain', 'level', 'significantli', 'better', 'significantli', 'better', '005first', 'shown', 'tabl', 'rerankingbi', 'appli', 'cslm42', 'increas', 'bleu', 'scoresfor', 'languag', 'model', 'observ', 'inaccord', 'previou', 'work', 'schwenk2010', 'huang', 'et', 'al', '2013second', 'rerank', 'result', 'bnlm42', '3244were', 'better', 'pa', 'ofbnlm746', 'indic', 'theunderli', 'bnlm', 'small', 'corpu', 'thererank', 'cslm', 'compens', 'it3w', 'code', 'avail', 'httpwwwarkcscmuedumtbnlm746rerankconv7461stconv42rerankbnlm7461stconv421stbnlm42rerankbnlm421stconv746', 'rerank', 'bnlm746', 'rerank', 'conv746', '1st', 'conv42', 'rerank', 'bnlm746', '1st', 'conv42', '1st', 'bnlm42', 'rerank', 'tabl', 'signific', 'test', 'system', 'differ', 'lmsthird', 'conv42', 'better', 'bnlm42', 'forboth', 'firstpass', 'rerank', 'hold', 'thecas', 'conv746', 'bnlm746', 'indicatedthat', 'convers', 'method', 'improv', 'bnlmseven', 'underli', 'bnlmwa', 'train', 'largercorpu', 'train', 'cslm', 'asdescrib', 'introduct', 'importantbecaus', 'bnlm', 'train', 'largercorpora', 'trainingcslm', 'observ', 'theprevi', 'workin', 'addit', 'firstpass', 'conv42', 'andconv746', 'compar', 'withthos', 'rerank', 'result', 'bnlm42', 'andbnlm746', 'respect', 'isther', 'signific', 'differ', 'theseresult', 'indic', 'convers', 'methodpreserv', 'perform', 'rerank', 'usingcslm5', 'conclusionw', 'propos', 'method', 'convert', 'cslmsinto', 'bnlm', 'method', 'improvea', 'bnlm', 'cslm', 'train', 'smallercorpu', 'train', 'bnlm', 'wehav', 'shown', 'bnlm', 'creat', 'methodperform', 'good', 'rerank', 'cslmsour', 'futur', 'work', 'compar', 'conversionmethod', 'arsoy', 'et', 'al', '201344we', 'awar', 'arsoy', 'et', 'al', 'compar', 'methodwith', 'ident', 'method', 'theexperi', 'conduct', 'speech', 'recognit', 'task', 'andth', 'scale', 'experi', 'larg', 'noticedtheir', 'work', 'submiss', 'paper', 'nothav', 'time', 'compar', 'method', 'method', 'smt848acknowledgmentsw', 'appreci', 'help', 'discuss', 'andrewfinch', 'paul', 'dixon', 'anonymousreview', 'invalu', 'comment', 'andsuggest', 'improv', 'paper', 'worki', 'support', 'nation', 'natur', 'sciencefound', 'china', 'grant', 'no61170114', 'nationalbas', 'research', 'program', 'china', 'grant', 'no2013cb329401', 'scienc', 'technologycommiss', 'shanghai', 'municip', 'grant', 'no13511500200referencesebru', 'arsoy', 'stanley', 'f', 'chen', 'bhuvana', 'ramabhadranand', 'abhinav', 'sethi', 'convert', 'neural', 'networklanguag', 'model', 'backoff', 'languag', 'model', 'foreffici', 'decod', 'automat', 'speech', 'recognitionin', 'proc', 'ieee', 'int', 'conf', 'acoust', 'speechand', 'signal', 'process', 'icassp', 'vancouvercanada', 'ieeeyoshua', 'bengio', 'rejean', 'ducharm', 'pascal', 'vincent', 'andchristian', 'janvin', 'neural', 'probabilisticlanguag', 'model', 'journal', 'machin', 'learningresearch', 'jmlr', 'marchstanley', 'f', 'chen', 'joshua', 'goodman', 'anempir', 'studi', 'smooth', 'techniqu', 'languagemodel', 'proceed', '34th', 'annual', 'meetingon', 'associ', 'comput', 'linguist', 'acl96', 'page', 'santa', 'cruz', 'california', 'juneassoci', 'comput', 'linguisticsstanley', 'f', 'chen', 'joshua', 'goodman', 'anempir', 'studi', 'smooth', 'techniqu', 'languagemodel', 'technic', 'report', 'scienc', 'groupharvard', 'univa', 'deora', 't', 'mikolov', 's', 'kombrink', 'm', 'karafiatand', 'sanjeev', 'khudanpur', 'variationalapproxim', 'longspan', 'languag', 'model', 'lvcsrin', 'acoust', 'speech', 'signal', 'process', 'icassp2011', 'ieee', 'intern', 'confer', 'page', 'pragu', 'czech', 'republ', 'ieeeatsushi', 'fujii', 'masao', 'utiyama', 'mikio', 'yamamoto', 'andtakehito', 'utsuro', 'overview', 'patenttransl', 'task', 'ntcir8', 'workshop', 'inproceed', '8th', 'ntcir', 'workshop', 'meetingon', 'evalu', 'inform', 'access', 'technologiesinform', 'retriev', 'question', 'answer', 'crosslingu', 'inform', 'access', 'page', 'tokyojapan', 'juneisao', 'goto', 'bin', 'lu', 'ka', 'po', 'chow', 'eiichiro', 'sumita', 'andbenjamin', 'k', 'tsou', 'overview', 'patentmachin', 'translat', 'task', 'ntcir9', 'workshopin', 'proceed', 'ntcir9', 'workshop', 'meet', 'pages559578', 'tokyo', 'japan', 'decemberzhongqiang', 'huang', 'jacob', 'devlin', 'spyrosmatsouka', 'bbn', 'system', 'chineseenglish', 'subtask', 'ntcir10', 'patentmt', 'evaluationin', 'ntcir10', 'tokyo', 'japan', 'junephilipp', 'koehn', 'franz', 'josef', 'och', 'daniel', 'marcu2003', 'statist', 'phrasebas', 'translat', 'inproceed', 'confer', 'thenorth', 'american', 'chapter', 'associ', 'forcomput', 'linguist', 'human', 'languagetechnolog', 'volum', 'naacl', 'page', '4854edmonton', 'canada', 'associ', 'computationallinguisticsphilipp', 'koehn', 'statist', 'signific', 'test', 'formachin', 'translat', 'evalu', 'dekang', 'lin', 'anddekai', 'wu', 'editor', 'proceed', 'emnlp', '2004page', 'barcelona', 'spain', 'juli', 'associationfor', 'comput', 'linguisticshaison', 'le', 'oparin', 'allauzen', 'j', 'gauvain', 'andf', 'yvon', 'structur', 'output', 'layer', 'neuralnetwork', 'languag', 'model', 'acoust', 'speech', 'andsign', 'process', 'icassp', 'ieee', 'internationalconfer', 'page', 'pragu', 'czechrepubl', 'ieeetoma', 'mikolov', 'anoop', 'deora', 'daniel', 'povey', 'lukasburget', 'jan', 'cernock', 'strategi', 'fortrain', 'larg', 'scale', 'neural', 'network', 'languag', 'modelsin', 'acoust', 'speech', 'signal', 'process', 'icassp2011', 'ieee', 'intern', 'confer', 'page', 'pragu', 'czech', 'republ', 'ieeejan', 'niehu', 'alex', 'waibel', 'continu', 'spacelanguag', 'model', 'restrict', 'boltzmann', 'machinesin', 'proceed', 'intern', 'workshop', 'forspoken', 'languag', 'translat', 'iwslt', 'pages311318', 'hong', 'kongfranz', 'josef', 'och', 'hermann', 'ney', 'systematiccomparison', 'statist', 'align', 'modelscomput', 'linguist', 'marchfranz', 'josef', 'och', 'minimum', 'error', 'ratetrain', 'statist', 'machin', 'translat', 'inproceed', '41st', 'annual', 'meet', 'theassoci', 'comput', 'linguist', 'pages160167', 'sapporo', 'japan', 'juli', 'associ', 'forcomput', 'linguisticskishor', 'papineni', 'salim', 'rouko', 'todd', 'ward', 'weij', 'zhu', 'bleu', 'method', 'automaticevalu', 'machin', 'translat', 'proceedingsof', '40th', 'annual', 'meet', 'associ', 'forcomput', 'linguist', 'acl', 'page', 'philadelphia', 'pennsylvania', 'june', 'associ', 'forcomput', 'linguisticsholg', 'schwenk', 'daniel', 'dchelott', 'jeanlucgauvain', 'continu', 'space', 'languag', 'modelsfor', 'statist', 'machin', 'translat', 'proceedingsof', 'colingacl', 'main', 'confer', 'postersess', 'colingacl', 'page', 'sydneyaustralia', 'juli', 'associ', 'computationallinguisticsholg', 'schwenk', 'anthoni', 'rousseau', 'mohammedattik', 'larg', 'prune', 'continu', 'spacelanguag', 'model', 'gpu', 'statist', 'machinetransl', 'proceed', 'naaclhlt', '2012workshop', 'replac', 'ngrammodel', 'futur', 'languagemodel', 'hltwlm', 'page', 'montreal', 'canada', 'juneassoci', 'comput', 'linguisticsholg', 'schwenk', 'continu', 'space', 'languagemodel', 'speech', 'languag', '213492518holger', 'schwenk', 'continuousspac', 'languagemodel', 'statist', 'machin', 'translat', 'praguebulletin', 'mathemat', 'linguist', 'page', '137146le', 'hai', 'son', 'alexandr', 'allauzen', 'guillaum', 'wisniewskiand', 'francoi', 'yvon', 'train', 'continuousspac', 'languag', 'model', 'practic', 'issu', 'inproceed', 'confer', 'empiricalmethod', 'natur', 'languag', 'process', 'emnlp10', 'page', 'cambridg', 'massachusettsoctob', 'associ', 'comput', 'linguisticsl', 'hai', 'son', 'alexandr', 'allauzen', 'francoi', 'yvon2012', 'continu', 'space', 'translat', 'model', 'withneur', 'network', 'proceed', '2012confer', 'north', 'american', 'chapter', 'theassoci', 'comput', 'linguist', 'humanlanguag', 'technolog', 'naacl', 'hlt', 'pages3948', 'montreal', 'canada', 'june', 'associ', 'forcomput', 'linguisticsandrea', 'stolck', 'srilman', 'extens', 'languagemodel', 'toolkit', 'proceed', 'internationalconfer', 'spoken', 'languag', 'process', 'pages257286', 'novemberomar', 'f', 'zaidan', 'zmert', 'fulli', 'configurableopen', 'sourc', 'tool', 'minimum', 'error', 'rate', 'train', 'ofmachin', 'translat', 'system', 'pragu', 'bulletin', 'ofmathemat', 'linguist']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text=\"\"\n",
        "for words in stem_text:\n",
        "        preprocessed_text+=words+\" \"\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAJ7GvW6JygW",
        "outputId": "ada188ec-eca8-4814-c3d9-8bca9c872cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "proceed confer empir method natur languag process page 845850seattl washington usa octob c2013 associ comput linguisticsconvert continuousspac languag model intongram languag model statist machin translationrui wang123 masao utiyama2 isao goto2 eiichro sumita2 hai zhao13 baoliang lu131 center brainlik comput machin intelligencedepart scienc engineeringshanghai jiao tong unviers shanghai china2 multilingu translat laboratori mastar projectn institut inform commun technology35 hikaridai keihanna scienc citi kyoto japan3 moemicrosoft key lab intellig comput intellig systemsshanghai jiao tong unviers shanghai chinawangruinlpgmailcom mutiyamaigotoeiichirosumitanictgojp zhaohaicssjtueducn bllusjtueducnabstractneur network languag model orcontinuousspac languag model cslmshave shown improv performanceof statist machin translat smtwhen rerank nbesttransl cslm notbeen pa decod smtbecaus cslm decod take lotof time contrast propos methodfor convert cslm backoff ngramlanguag model bnlm canus convert cslm decod showthat outperform origin bnlm andar compar tradit use ofcslm reranking1 introductionlanguag model import natur languageprocess task speech recognit andstatist machin translat tradit backoff ngram languag model bnlm chen andgoodman chen goodman 1998stolck wide tasksrec neural network languag modelsor continuousspac languag model cslmsbengio et al schwenk le et al 2011are statist machin translationsmt schwenk et al son et al 2010schwenk et al son et al niehuesand waibel work shown thatcslm improv bleu papineni et al2002 score smt compar bnlmson condit train data languagemodel size practicecslm wide smtone reason comput cost oftrain cslm high variousmethod propos tackl trainingcost issu son et al schwenk et al 2012mikolov et al littlework reduc cost costsof cslm high difficult use cslmsin decod directlya common approach smt cslm isth pa approach nbest rerank thisapproach pa us bnlm decodingto produc nbest list cslm torerank nbest translat second passschwenk et al son et al schwenk etal son et al 2012anoth approach restrict boltzmannmachin rbm niehu waibel 2012instead multilay neural networksbengio et al schwenk le et al2011 probabl rbm calculatedveri effici niehu waibel theycan use rbm languag model smt decodinghowev rbm adapt ofsmt larg smt task trainingcost rbm highth approach bnlm simulatea cslm deora et al arsoy et al 2013deora et al recurr neural networklanguag model rnnlm gener largeamount text gener samplingword probabl distribut calculatedbi rnnlm train bnlm845from text interpol kneserneysmooth method arsoy et al convertedneur network languag model increas orderto prune backoff languag model lowerord model constrain ngram allow inhigherord modelsboth method decod forspeech recognit method appliedto notsolarg scale experi million mword train bnlm arsoy et al2013 contrast method appli smtand improv bnlm creat from746 m word cslm train mwordsbecaus bnlm train largercorpora trainingcslm improv bnlm cslmtrain smaller corpu importantactu cslm train smaller corpuscan improv bleu score smt usedin nbest rerank schwenk huang etal contrast demonstr abnlm simul cslm improv bleuscor smt pa decodingour approach follow train acslm schwenk corpu secondw train bnlm corpu orlarg corpu final rewrit probabilityof ngram bnlm probabilitycalcul cslmwe renorm theprob bnlm use rewrittenbnlm smt decodingin section bnlm cslmschwenk rewrit bnlm insect method convertinga cslm bnlm section weevalu method conclude2 languag modelsin section introduc standardbnlm cslm structur probabilitycalculation21 standard backoff ngram languag modela bnlm predict probabl wordwi givenit preced n word hi wi1in1 butit suffer data spars contexthi appear train data anestim backingoff model smallerhistori necessari case modifiedkneserney smooth chen goodman 1998the probabl wi given hi bnlmpbwihi ispbwihi pbwihi hipbwiwi1in2 1where pbwihi discount probabl andhi backoff weight bnlm witha cslm shown below22 cslm structur probabilitycalculationth main structur cslm multilay neural network contain layer inputlay project word context hi ontoth project layer hidden layer thesecond hidden layer output layer achiev thenonlin probabl estim calcul thelanguag model probabl p wihi givencontext schwenk 2007the cslm calcul probabl allword vocabulari corpu giventh context thecomput complex calcul theprob word high cslm isonli calcul probabl subsetof vocabulari subset calleda shortlist consist frequentword vocabulari cslm calculatesth sum probabl word theshortlist assign neuron purposeth probabl word shortlistar obtain bnlm schwenk 2007schwenk 2010let wi hi current word histori thecslm bnlm calcul probabl ofwi given hi p wihi followsp wihi pcwihi1pcohipshi wi shortlistpbwihi otherwise2wher pc probabl calcul thecslm pcohi probabl neuronfor word shortlist pb theprob calcul bnlm eq 1andpshi vshortlistpbvhi 3846it consid cslm redistributesth probabl mass word shortlistthi probabl mass calcul thebnlm3 convers cslm bnlma describ introduct train acslm corpu train bnlm fromth corpu larger corpu rewriteth probabl ngram bnlm theprob calcul cslmfirst use probabl 1gram inth bnlm rewrit theprob ngram n2345 bnlmwith probabl calcul ngramcslm respect note ngram cslmmean length histori n notealso need rewrit probabilitiesof ngram end word shortlistfin renorm probabl thebnlm srilm renorm optionwhen rewrit bnlm train largercorpu ngram bnlm containunknown word cslm case useth probabl bnlm are4 experiments41 common settingsw patent data chines englishpat translat subtask ntcir9 patenttransl task goto et al paralleltrain develop test data consist 1m sentenc respectivelyw follow set ntcir9 chineseto english translat baselin goto et al2011 languag modelsto compar mose phrasebas smt koehn et al togetherwith giza och ney align andmert och tune developmentdata translat perform measur byth caseinsensit bleu score tokenizedtest data mtevalv13apl forcalcul bleu scores11it avail httpwwwitlnistgoviadmigtestsmt2009w standard smt featur fivetransl model score word penalti scoreseven distort score languag modelscor differ languag model wasus calcul languag model scorea baselin bnlm train 5grambnlm modifi kneserney smooth usingth english m sentenc train datawhich consist m word discardani ngram train model wedid use count cutoff bnlm asbnlm42a 5gram cslm train same1 m train sentenc cslm toolkitschwenk set cslmwere project layer dimens eachword hidden layer dimens outputlay shortlist dimens wererecommend cslm toolkit thiscslm cslm42 cslm42 bnlm42 thebackground bnlmwe train larger 5gram bnlm withmodifi kneserney smooth addingsent patent data distributedin ntcir8 patent translat task fujii et al2010 m word data consist of746 m word bnlm bnlm746 wediscard 345gram occur whenw creat bnlm746next rewrot bnlm42 cslm42 byus method describ section thisrewritten bnlm interpol bnlm42the interpol weight determin gridsearch chang interpol weightto creat interpolatedbnlm bnlm smtsystem tune weight paramet firsthalf develop data selectedth interpol weight obtain highestbleu score second half developmentdata select interpol weightw appli mert sentencedevelop data tune weight parameters2w bnlm conv42 obtainedconv746 rewrit bnlm746 cslm422we awar interpol weight bedetermin minim perplex developmentdata opt directli maxim bleu score847in wayth vocabulari languag model thesam extract m trainingsentences42 experiment resultst show percent bleu score testdata figur 1st pa column showth bleu score pa decod whenw chang languag model figur thererank column bleu score whenw appli cslm42 rerank 100best list forth differ languag model appliedcslm42 rerank ad cslm42score addit 15th featur weightparamet tune zmert zaidan2009lm 1st pa rerankbnlm42 3244conv42 3298bnlm746 3336conv746 3354tabl comparison bleu scoresw perform pair bootstrap resampl test koehn sampl 2000sampl signific testtabl show result statisticalsignific test 1st short forth 1st pa mark indic thelm left mark significantli betterthan mark certain level significantli better significantli better 005first shown tabl rerankingbi appli cslm42 increas bleu scoresfor languag model observ inaccord previou work schwenk2010 huang et al 2013second rerank result bnlm42 3244were better pa ofbnlm746 indic theunderli bnlm small corpu thererank cslm compens it3w code avail httpwwwarkcscmuedumtbnlm746rerankconv7461stconv42rerankbnlm7461stconv421stbnlm42rerankbnlm421stconv746 rerank bnlm746 rerank conv746 1st conv42 rerank bnlm746 1st conv42 1st bnlm42 rerank tabl signific test system differ lmsthird conv42 better bnlm42 forboth firstpass rerank hold thecas conv746 bnlm746 indicatedthat convers method improv bnlmseven underli bnlmwa train largercorpu train cslm asdescrib introduct importantbecaus bnlm train largercorpora trainingcslm observ theprevi workin addit firstpass conv42 andconv746 compar withthos rerank result bnlm42 andbnlm746 respect isther signific differ theseresult indic convers methodpreserv perform rerank usingcslm5 conclusionw propos method convert cslmsinto bnlm method improvea bnlm cslm train smallercorpu train bnlm wehav shown bnlm creat methodperform good rerank cslmsour futur work compar conversionmethod arsoy et al 201344we awar arsoy et al compar methodwith ident method theexperi conduct speech recognit task andth scale experi larg noticedtheir work submiss paper nothav time compar method method smt848acknowledgmentsw appreci help discuss andrewfinch paul dixon anonymousreview invalu comment andsuggest improv paper worki support nation natur sciencefound china grant no61170114 nationalbas research program china grant no2013cb329401 scienc technologycommiss shanghai municip grant no13511500200referencesebru arsoy stanley f chen bhuvana ramabhadranand abhinav sethi convert neural networklanguag model backoff languag model foreffici decod automat speech recognitionin proc ieee int conf acoust speechand signal process icassp vancouvercanada ieeeyoshua bengio rejean ducharm pascal vincent andchristian janvin neural probabilisticlanguag model journal machin learningresearch jmlr marchstanley f chen joshua goodman anempir studi smooth techniqu languagemodel proceed 34th annual meetingon associ comput linguist acl96 page santa cruz california juneassoci comput linguisticsstanley f chen joshua goodman anempir studi smooth techniqu languagemodel technic report scienc groupharvard univa deora t mikolov s kombrink m karafiatand sanjeev khudanpur variationalapproxim longspan languag model lvcsrin acoust speech signal process icassp2011 ieee intern confer page pragu czech republ ieeeatsushi fujii masao utiyama mikio yamamoto andtakehito utsuro overview patenttransl task ntcir8 workshop inproceed 8th ntcir workshop meetingon evalu inform access technologiesinform retriev question answer crosslingu inform access page tokyojapan juneisao goto bin lu ka po chow eiichiro sumita andbenjamin k tsou overview patentmachin translat task ntcir9 workshopin proceed ntcir9 workshop meet pages559578 tokyo japan decemberzhongqiang huang jacob devlin spyrosmatsouka bbn system chineseenglish subtask ntcir10 patentmt evaluationin ntcir10 tokyo japan junephilipp koehn franz josef och daniel marcu2003 statist phrasebas translat inproceed confer thenorth american chapter associ forcomput linguist human languagetechnolog volum naacl page 4854edmonton canada associ computationallinguisticsphilipp koehn statist signific test formachin translat evalu dekang lin anddekai wu editor proceed emnlp 2004page barcelona spain juli associationfor comput linguisticshaison le oparin allauzen j gauvain andf yvon structur output layer neuralnetwork languag model acoust speech andsign process icassp ieee internationalconfer page pragu czechrepubl ieeetoma mikolov anoop deora daniel povey lukasburget jan cernock strategi fortrain larg scale neural network languag modelsin acoust speech signal process icassp2011 ieee intern confer page pragu czech republ ieeejan niehu alex waibel continu spacelanguag model restrict boltzmann machinesin proceed intern workshop forspoken languag translat iwslt pages311318 hong kongfranz josef och hermann ney systematiccomparison statist align modelscomput linguist marchfranz josef och minimum error ratetrain statist machin translat inproceed 41st annual meet theassoci comput linguist pages160167 sapporo japan juli associ forcomput linguisticskishor papineni salim rouko todd ward weij zhu bleu method automaticevalu machin translat proceedingsof 40th annual meet associ forcomput linguist acl page philadelphia pennsylvania june associ forcomput linguisticsholg schwenk daniel dchelott jeanlucgauvain continu space languag modelsfor statist machin translat proceedingsof colingacl main confer postersess colingacl page sydneyaustralia juli associ computationallinguisticsholg schwenk anthoni rousseau mohammedattik larg prune continu spacelanguag model gpu statist machinetransl proceed naaclhlt 2012workshop replac ngrammodel futur languagemodel hltwlm page montreal canada juneassoci comput linguisticsholg schwenk continu space languagemodel speech languag 213492518holger schwenk continuousspac languagemodel statist machin translat praguebulletin mathemat linguist page 137146le hai son alexandr allauzen guillaum wisniewskiand francoi yvon train continuousspac languag model practic issu inproceed confer empiricalmethod natur languag process emnlp10 page cambridg massachusettsoctob associ comput linguisticsl hai son alexandr allauzen francoi yvon2012 continu space translat model withneur network proceed 2012confer north american chapter theassoci comput linguist humanlanguag technolog naacl hlt pages3948 montreal canada june associ forcomput linguisticsandrea stolck srilman extens languagemodel toolkit proceed internationalconfer spoken languag process pages257286 novemberomar f zaidan zmert fulli configurableopen sourc tool minimum error rate train ofmachin translat system pragu bulletin ofmathemat linguist \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLPbmahfw9Vj"
      },
      "source": [
        "query_vec = vectorizer.transform([query_doc])\n",
        "results = cosine_similarity(X,query_vec).reshape((-1,)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0400KSFAhpl5",
        "outputId": "aadc5b1c-6f50-48d0-c9bc-249e78452bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4853565  0.53779828 0.96827556 0.50216002 0.52383612 0.50792065\n",
            " 0.42481958 0.37689999 0.51669911 0.20953878 0.5719123  0.54375312\n",
            " 0.47639931 0.41435562 0.53462006 0.54272806 0.52287545 0.47721517\n",
            " 0.36620436 0.47759982 0.57083238 0.54412869 0.51954543 0.60750377\n",
            " 0.52981661 0.53065637 0.47969079 0.49058118 0.50461928 0.50594433\n",
            " 0.55722514 0.58937546 0.47723737 0.49944494 0.50556763 0.60065354\n",
            " 0.56252012 0.48713991 0.56488948 0.46338865]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sort_order = (sorted(range(len(results)), key = results.__getitem__))\n",
        "sort_order.reverse()\n",
        "sorted_docs = []\n",
        "for i in sort_order:\n",
        "    sorted_docs.append(docs[i])"
      ],
      "metadata": {
        "id": "wsxUPHEMi-78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "dc84a628-e98c-44d0-cf0a-e2c18f94402f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d0e8d8f6872d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msorted_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msort_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msorted_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "6Z1pqZMjkiS0",
        "outputId": "71b9aa66-9f6d-49ff-ff09-c540fae14e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 845\\x96850,\\nSeattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics\\nConverting Continuous-Space Language Models into\\nN-gram Language Models for Statistical Machine Translation\\nRui Wang1,2,3, Masao Utiyama2, Isao Goto2, Eiichro Sumita2, Hai Zhao1,3 and Bao-Liang Lu1,3\\n1 Center for Brain-Like Computing and Machine Intelligence,\\nDepartment of Computer Science and Engineering,\\nShanghai Jiao Tong Unviersity, Shanghai, 200240, China\\n2 Multilingual Translation Laboratory, MASTAR Project,\\nNational Institute of Information and Communications Technology\\n3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan\\n3 MOE-Microsoft Key Lab. for Intelligent Computing and Intelligent Systems\\nShanghai Jiao Tong Unviersity, Shanghai 200240 China\\nwangrui.nlp@gmail.com, mutiyama/igoto/eiichiro.sumita@nict.go.jp, zhaohai@cs.sjtu.edu.cn, bllu@sjtu.edu.cn\\nAbstract\\nNeural network language models, or\\ncontinuous-space language models (CSLMs),\\nhave been shown to improve the performance\\nof statistical machine translation (SMT)\\nwhen they are used for reranking n-best\\ntranslations. However, CSLMs have not\\nbeen used in the first pass decoding of SMT,\\nbecause using CSLMs in decoding takes a lot\\nof time. In contrast, we propose a method\\nfor converting CSLMs into back-off n-gram\\nlanguage models (BNLMs) so that we can\\nuse converted CSLMs in decoding. We show\\nthat they outperform the original BNLMs and\\nare comparable with the traditional use of\\nCSLMs in reranking.\\n1 Introduction\\nLanguage models are important in natural language\\nprocessing tasks such as speech recognition and\\nstatistical machine translation. Traditionally, back-\\noff n-gram language models (BNLMs) (Chen and\\nGoodman, 1996; Chen and Goodman, 1998;\\nStolcke, 2002) are being widely used for these tasks.\\nRecently, neural network language models,\\nor continuous-space language models (CSLMs)\\n(Bengio et al., 2003; Schwenk, 2007; Le et al., 2011)\\nare being used in statistical machine translation\\n(SMT) (Schwenk et al., 2006; Son et al., 2010;\\nSchwenk et al., 2012; Son et al., 2012; Niehues\\nand Waibel, 2012). These works have shown that\\nCSLMs can improve the BLEU (Papineni et al.,\\n2002) scores of SMT when compared with BNLMs,\\non the condition that the training data for language\\nmodeling are the same size. However, in practice,\\nCSLMs have not been widely used in SMT.\\nOne reason is that the computational costs of\\ntraining and using CSLMs are very high. Various\\nmethods have been proposed to tackle the training\\ncost issues (Son et al., 2010; Schwenk et al., 2012;\\nMikolov et al., 2011). However, there has been little\\nwork on reducing using costs. Since the using costs\\nof CSLMs are very high, it is difficult to use CSLMs\\nin decoding directly.\\nA common approach in SMT using CSLMs is\\nthe two pass approach, or n-best reranking. In this\\napproach, the first pass uses a BNLM in decoding\\nto produce an n-best list. Then, a CSLM is used to\\nrerank those n-best translations in the second pass.\\n(Schwenk et al., 2006; Son et al., 2010; Schwenk et\\nal., 2012; Son et al., 2012)\\nAnother approach is using restricted Boltzmann\\nmachines (RBMs) (Niehues and Waibel, 2012)\\ninstead of using multi-layer neural networks\\n(Bengio et al., 2003; Schwenk, 2007; Le et al.,\\n2011). Since probability in a RBM can be calculated\\nvery efficiently (Niehues and Waibel, 2012), they\\ncan use the RBM language model in SMT decoding.\\nHowever, the RBM was just used in an adaptation of\\nSMT, not in a large SMT task, because the training\\ncosts of RBMs are very high.\\nThe last approach is using a BNLM to simulate\\na CSLM (Deoras et al., 2011; Arsoy et al., 2013).\\n(Deoras et al., 2011) used a recurrent neural network\\nlanguage model (RNNLM) to generate a large\\namount of text, which was generated by sampling\\nwords from the probability distributions calculated\\nby the RNNLM. Then, they trained the BNLM\\n845\\nfrom the text using the interpolated Kneser-Ney\\nsmoothing method. (Arsoy et al., 2013) converted\\nneural network language models of increasing order\\nto pruned back-off language models, using lower-\\norder models to constrain the n-grams allowed in\\nhigher-order models.\\nBoth of these methods were used in decoding for\\nspeech recognition. These methods were applied\\nto not-so-large scale experiments (55 million (M)\\nwords for training their BNLMs) (Arsoy et al.,\\n2013). In contrast, our method is applied to SMT\\nand can be used to improve a BNLM created from\\n746 M words by using a CSLM trained from 42 M\\nwords.\\nBecause BNLMs can be trained from much larger\\ncorpora than those that can be used for training\\nCSLMs, improving a BNLM by using a CSLM\\ntrained from a smaller corpus is very important.\\nActually, a CSLM trained from a smaller corpus\\ncan improve the BLEU scores of SMT if it is used\\nin the n-best reranking (Schwenk, 2010; Huang et\\nal., 2013). In contrast, we will demonstrate that a\\nBNLM simulating a CSLM can improve the BLEU\\nscores of SMT in the first pass decoding.\\nOur approach is as follows: (1) First, we train a\\nCSLM (Schwenk, 2007) from a corpus. (2) Second,\\nwe also train a BNLM from the same corpus or\\nlarger corpus. (3) Finally, we rewrite the probability\\nof each n-gram of the BNLM with that probability\\ncalculated from the CSLM.We also re-normalize the\\nprobabilities of the BNLM, then use the re-written\\nBNLM in SMT decoding.\\nIn Section 2, we describe the BNLM and CSLM\\n(Schwenk, 2010) used for re-writing BNLMs. In\\nSection 3, we describe the method of converting\\na CSLM into a BNLM. In Sections 4 and 5, we\\nevaluate our method and conclude.\\n2 Language Models\\nIn this section, we will introduce the standard\\nBNLM and CSLM structure and probability\\ncalculation.\\n2.1 Standard back-off ngram language model\\nA BNLM predicts the probability of a wordwi given\\nits preceding n ? 1 words hi = wi?1i?n+1. But\\nit will suffer from data sparseness if the context,\\nhi, does not appear in the training data. So an\\nestimation by \\x93backing-off\\x94 to models with smaller\\nhistories is necessary. In the case of the modified\\nKneser-Ney smoothing (Chen and Goodman, 1998),\\nthe probability of wi given hi under a BNLM,\\nPb(wi|hi), is:\\nPb(wi|hi) = P\\x88b(wi|hi) + ?(hi)Pb(wi|wi?1i?n+2) (1)\\nwhere P\\x88b(wi|hi) is a discounted probability and\\n?(hi) is the back-off weight. A BNLM is used with\\na CSLM as shown below.\\n2.2 CSLM structure and probability\\ncalculation\\nThe main structure of a CSLM using a multi-\\nlayer neural network contains four layers: the input\\nlayer projects all words in the context hi onto\\nthe projection layer (the first hidden layer); the\\nsecond hidden layer and the output layer achieve the\\nnon-liner probability estimation and calculate the\\nlanguage model probability P (wi|hi) for the given\\ncontext. (Schwenk, 2007).\\nThe CSLM calculates the probabilities of all\\nwords in the vocabulary of the corpus given\\nthe context at once. However, because the\\ncomputational complexity of calculating the\\nprobabilities of all words is quite high, the CSLM is\\nonly used to calculate the probabilities of a subset\\nof the whole vocabulary. This subset is called\\na short-list, which consists of the most frequent\\nwords in the vocabulary. The CSLM also calculates\\nthe sum of the probabilities of all words not in the\\nshort-list by assigning a neuron for that purpose.\\nThe probabilities of other words not in the short-list\\nare obtained from a BNLM (Schwenk, 2007;\\nSchwenk, 2010).\\nLet wi, hi be the current word and history. The\\nCSLM with a BNLM calculates the probability of\\nwi given hi, P (wi|hi), as follows:\\nP (wi|hi) =\\n{\\nPc(wi|hi)\\n1?Pc(o|hi)\\nPs(hi) if wi ? short-list\\nPb(wi|hi) otherwise\\n(2)\\nwhere Pc(·) is the probability calculated by the\\nCSLM, Pc(o|hi) is the probability of the neuron\\nfor the words not in the short-list, Pb(·) is the\\nprobability calculated by the BNLM as in Eq. 1,\\nand\\nPs(hi) =\\n?\\nv?short-list\\nPb(v|hi). (3)\\n846\\nIt can be considered that the CSLM redistributes\\nthe probability mass of all words in the short-list.\\nThis probability mass is calculated by using the\\nBNLM.\\n3 Conversion of CSLM into BNLM\\nAs described in the introduction, we first train a\\nCSLM from a corpus. We also train a BNLM from\\nthe same corpus or a larger corpus. Then, we rewrite\\nthe probability of each ngram in the BNLM with the\\nprobability calculated from the CSLM.\\nFirst, we use the probabilities of 1-grams in\\nthe BNLM as they are. Next, we rewrite the\\nprobabilities of n-grams (n=2,3,4,5) in the BNLM\\nwith the probabilities calculated by using the n-gram\\nCSLM, respectively. Note that the n-gram CSLM\\nmeans that the length of its history is n ? 1. Note\\nalso that we only need to rewrite the probabilities\\nof n-grams ending with a word in the short-list.\\nFinally, we re-normalize the probabilities of the\\nBNLM using the SRILM\\x92s \\x91-renorm\\x92 option.\\nWhen we rewrite a BNLM trained from a larger\\ncorpus, the ngrams in the BNLM often contain\\nunknown words for the CSLM. In that case, we use\\nthe probabilities in the BNLM as they are.\\n4 Experiments\\n4.1 Common settings\\nWe used the patent data for the Chinese to English\\npatent translation subtask from the NTCIR-9 patent\\ntranslation task (Goto et al., 2011). The parallel\\ntraining, development, and test data consisted of 1\\nM, 2,000, and 2,000 sentences, respectively.\\nWe followed the settings of the NTCIR-9 Chinese\\nto English translation baseline system (Goto et al.,\\n2011) except that we used various language models\\nto compare them. We used the MOSES phrase-\\nbased SMT system (Koehn et al., 2003), together\\nwith Giza++ (Och and Ney, 2003) for alignment and\\nMERT (Och, 2003) for tuning on the development\\ndata. The translation performance was measured by\\nthe case-insensitive BLEU scores on the tokenized\\ntest data. We used mteval-v13a.pl for\\ncalculating BLEU scores.1\\n1It is available at http://www.itl.nist.gov/iad/\\nmig/tests/mt/2009/\\nWe used the 14 standard SMT features: five\\ntranslation model scores, one word penalty score,\\nseven distortion scores and one language model\\nscore. Each of the different language models was\\nused to calculate the language model score.\\nAs the baseline BNLM, we trained a 5-gram\\nBNLM with modified Kneser-Ney smoothing using\\nthe English side of the 1 M sentences training data,\\nwhich consisted of 42 M words. We did not discard\\nany n-grams in training this model. That is, we\\ndid not use count cutoffs. We call this BNLM as\\nBNLM42.\\nA 5-gram CSLM was trained on the same\\n1 M training sentences using the CSLM toolkit\\n(Schwenk, 2010). The settings for the CSLM\\nwere: projection layer of dimension 256 for each\\nword, hidden layer of dimension 384 and output\\nlayer (short-list) of dimension 8192, which were\\nrecommended in the CSLM toolkit. We call this\\nCSLM CSLM42. CSLM42 used BNLM42 as the\\nbackground BNLM.\\nWe also trained a larger 5-gram BNLM with\\nmodified Kneser-Ney smoothing by adding\\nsentences from the 2005 US patent data distributed\\nin the NTCIR-8 patent translation task (Fujii et al.,\\n2010) to the 42 M words. The data consisted of\\n746 M words. We call this BNLM BNLM746. We\\ndiscarded 3,4,5-grams that occurred only once when\\nwe created BNLM746.\\nNext, we re-wrote BNLM42 with CSLM42 by\\nusing the method described in Section 3. This\\nre-written BNLM was interpolated with BNLM42.\\nThe interpolation weight was determined by the grid\\nsearch. That is, we changed the interpolation weight\\nto 0.1, 0.3, 0.5, 0.7, 0.9 to create an interpolated\\nBNLM. Then we used that BNLM in the SMT\\nsystem to tune the weight parameters on the first\\nhalf of the development data. Next, we selected\\nthe interpolation weight that obtained the highest\\nBLEU score on the second half of the development\\ndata. After we selected the interpolation weight,\\nwe applied MERT again to the 2,000 sentence\\ndevelopment data to tune the weight parameters.2\\nWe call this BNLM CONV42. We also obtained\\nCONV746 by re-writing BNLM746 with CSLM42\\n2We aware that the interpolation weight might be\\ndetermined by minimizing the perplexity on the development\\ndata. However, we opted to directly maximize the BLEU score.\\n847\\nin the same way.\\nThe vocabulary of these language models was the\\nsame, which was extracted from the 1 M training\\nsentences.\\n4.2 Experimental results\\nTable 1 shows the percent BLEU scores on the test\\ndata. The figures in the \\x931st pass\\x94 column show\\nthe BLEU scores in the first pass decoding when\\nwe changed the language model. The figures in the\\n\\x93reranking\\x94 column show the BLEU scores when\\nwe applied CSLM42 to rerank the 100-best lists for\\nthe different language models. When we applied\\nCSLM42 for reranking, we added the CSLM42\\nscore as the additional 15th feature. The weight\\nparameters were tuned by using Z-MERT (Zaidan,\\n2009).\\nLMs 1st pass rerank\\nBNLM42 31.60 32.44\\nCONV42 32.58 32.98\\nBNLM746 32.83 33.36\\nCONV746 33.22 33.54\\nTable 1: Comparison of BLEU scores\\nWe also performed the paired bootstrap re-\\nsampling test (Koehn, 2004).3 We sampled 2000\\nsamples for each significance test.\\nTable 2 shows the results of a statistical\\nsignificance test, in which the \\x931st\\x94 is short for\\nthe \\x931st pass\\x94. The marks indicate whether the\\nLM to the left of a mark is significantly better\\nthan that above the mark at a certain level. (\\x93?\\x94:\\nsignificantly better at ? = 0.01, \\x93>\\x94: ? = 0.05,\\n\\x93?\\x94: not significantly better at ? = 0.05)\\nFirst, as shown in the tables, the reranking\\nby applying CSLM42 increased the BLEU scores\\nfor all language models. This observation is in\\naccordance with those of previous work (Schwenk,\\n2010; Huang et al., 2013).\\nSecond, the reranking results of BNLM42 (32.44)\\nwere not better than those of the first pass of\\nBNLM746 (32.83). This indicates that if the\\nunderlying BNLM is made from a small corpus, the\\nreranking using CSLM can not compensate for it.\\n3We used the code available at http://www.ark.cs.\\ncmu.edu/MT/.\\nBN\\nLM\\n74\\n6\\n(re\\nra\\nnk\\n)\\nCO\\nN\\nV\\n74\\n6\\n(1\\nst)\\nCO\\nN\\nV\\n42\\n(re\\nra\\nnk\\n)\\nBN\\nLM\\n74\\n6\\n(1\\nst)\\nCO\\nN\\nV\\n42\\n(1\\nst)\\nBN\\nLM\\n42\\n(re\\nra\\nnk\\n)\\nBN\\nLM\\n42\\n(1\\nst)\\nCONV746 (rerank) ? ? ? ? ? ? ?\\nBNLM746 (rerank) ? ? > ? ? ?\\nCONV746 (1st) ? ? ? ? ?\\nCONV42 (rerank) ? ? ? ?\\nBNLM746 (1st) ? ? ?\\nCONV42 (1st) ? ?\\nBNLM42 (rerank) ?\\nTable 2: Significance tests for systems with different LMs\\nThird, CONV42 was better than BNLM42 for\\nboth first-pass and reranking. This also holds in the\\ncase of CONV746 and BNLM746. This indicated\\nthat our conversion method improved the BNLMs,\\neven if the underlying BNLMwas trained on a larger\\ncorpus than that used for training the CSLM. As\\ndescribed in the introduction, this is very important\\nbecause BNLMs can be trained from much larger\\ncorpora than those that can be used for training\\nCSLMs. This observation has not been found in the\\nprevious work.\\nIn addition, the first-pass of CONV42 and\\nCONV746 (32.58 and 33.22) were comparable with\\nthose of the reranking results of BNLM42 and\\nBNLM746 (32.44 and 33.36), respectively. That is,\\nthere were no significant differences between these\\nresults. This indicates that our conversion method\\npreserves the performance of the reranking using\\nCSLM.\\n5 Conclusion\\nWe have proposed a method for converting CSLMs\\ninto BNLMs. The method can be used to improve\\na BNLM by using a CSLM trained from a smaller\\ncorpus than that used for training the BNLM. We\\nhave also shown that BNLMs created by our method\\nperforms as good as the reranking using CSLMs.\\nOur future work is to compare our conversion\\nmethod with that of (Arsoy et al., 2013).4\\n4We aware that (Arsoy et al., 2013) compared their method\\nwith the one that is identical with our method. However, the\\nexperiments were conducted on a speech recognition task and\\nthe scale of the experiment was not so large. Since we noticed\\ntheir work just before the submission of our paper, we did not\\nhave time to compare their method with our method in SMT.\\n848\\nAcknowledgments\\nWe appreciate the helpful discussion with Andrew\\nFinch and Paul Dixon, and three anonymous\\nreviewers for many invaluable comments and\\nsuggestions to improve our paper. This work\\nis supported by the National Natural Science\\nFoundation of China (Grant No. 60903119, No.\\n61170114 and No. 61272248), the National\\nBasic Research Program of China (Grant No.\\n2013CB329401) and the Science and Technology\\nCommission of Shanghai Municipality (Grant No.\\n13511500200).\\nReferences\\nEbru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,\\nand Abhinav Sethy. 2013. Converting neural network\\nlanguage models into back-off language models for\\nefficient decoding in automatic speech recognition.\\nIn Proc. of IEEE Int. Conf. on Acoustics, Speech\\nand Signal Processing (ICASSP 2013), Vancouver,\\nCanada, May. IEEE.\\nYoshua Bengio, Re´jean Ducharme, Pascal Vincent, and\\nChristian Janvin. 2003. A neural probabilistic\\nlanguage model. Journal of Machine Learning\\nResearch (JMLR), 3:1137\\x961155, March.\\nStanley F. Chen and Joshua Goodman. 1996. An\\nempirical study of smoothing techniques for language\\nmodeling. In Proceedings of the 34th annual meeting\\non Association for Computational Linguistics, ACL\\n\\x9296, pages 310\\x96318, Santa Cruz, California, June.\\nAssociation for Computational Linguistics.\\nStanley F. Chen and Joshua Goodman. 1998. An\\nempirical study of smoothing techniques for language\\nmodeling. Technical report, Computer Science Group,\\nHarvard Univ.\\nA. Deoras, T. Mikolov, S. Kombrink, M. Karafiat,\\nand Sanjeev Khudanpur. 2011. Variational\\napproximation of long-span language models for lvcsr.\\nIn Acoustics, Speech and Signal Processing (ICASSP),\\n2011 IEEE International Conference on, pages 5532\\x96\\n5535, Prague, Czech Republic, May. IEEE.\\nAtsushi Fujii, Masao Utiyama, Mikio Yamamoto, and\\nTakehito Utsuro. 2010. Overview of the patent\\ntranslation task at the ntcir-8 workshop. In In\\nProceedings of the 8th NTCIR Workshop Meeting\\non Evaluation of Information Access Technologies:\\nInformation Retrieval, Question Answering and Cross-\\nlingual Information Access, pages 293\\x96302, Tokyo,\\nJapan, June.\\nIsao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and\\nBenjamin K. Tsou. 2011. Overview of the patent\\nmachine translation task at the NTCIR-9 workshop.\\nIn Proceedings of NTCIR-9 Workshop Meeting, pages\\n559\\x96578, Tokyo, Japan, December.\\nZhongqiang Huang, Jacob Devlin, and Spyros\\nMatsoukas. 2013. Bbn\\x92s systems for the chinese-\\nenglish sub-task of the ntcir-10 patentmt evaluation.\\nIn NTCIR-10, Tokyo, Japan, June.\\nPhilipp Koehn, Franz Josef Och, and Daniel Marcu.\\n2003. Statistical phrase-based translation. In\\nProceedings of the 2003 Conference of the\\nNorth American Chapter of the Association for\\nComputational Linguistics on Human Language\\nTechnology - Volume 1, NAACL \\x9203, pages 48\\x9654,\\nEdmonton, Canada. Association for Computational\\nLinguistics.\\nPhilipp Koehn. 2004. Statistical significance tests for\\nmachine translation evaluation. In Dekang Lin and\\nDekai Wu, editors, Proceedings of EMNLP 2004,\\npages 388\\x96395, Barcelona, Spain, July. Association\\nfor Computational Linguistics.\\nHai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and\\nF. Yvon. 2011. Structured output layer neural\\nnetwork language model. In Acoustics, Speech and\\nSignal Processing (ICASSP), 2011 IEEE International\\nConference on, pages 5524\\x965527, Prague, Czech\\nRepublic, May. IEEE.\\nTomas Mikolov, Anoop Deoras, Daniel Povey, Lukas\\nBurget, and Jan Cernock. 2011. Strategies for\\ntraining large scale neural network language models.\\nIn Acoustics, Speech and Signal Processing (ICASSP),\\n2011 IEEE International Conference on, pages 196\\x96\\n201, Prague, Czech Republic, May. IEEE.\\nJan Niehues and Alex Waibel. 2012. Continuous space\\nlanguage models using restricted boltzmann machines.\\nIn Proceedings of the International Workshop for\\nSpoken Language Translation, IWSLT 2012, pages\\n311\\x96318, Hong Kong.\\nFranz Josef Och and Hermann Ney. 2003. A systematic\\ncomparison of various statistical alignment models.\\nComputational Linguistics, 29(1):19\\x9651, March.\\nFranz Josef Och. 2003. Minimum error rate\\ntraining in statistical machine translation. In\\nProceedings of the 41st Annual Meeting of the\\nAssociation for Computational Linguistics, pages\\n160\\x96167, Sapporo, Japan, July. Association for\\nComputational Linguistics.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings\\nof the 40th Annual Meeting on Association for\\nComputational Linguistics, ACL \\x9202, pages 311\\x96\\n849\\n318, Philadelphia, Pennsylvania, June. Association for\\nComputational Linguistics.\\nHolger Schwenk, Daniel Dchelotte, and Jean-Luc\\nGauvain. 2006. Continuous space language models\\nfor statistical machine translation. In Proceedings\\nof the COLING/ACL on Main conference poster\\nsessions, COLING-ACL \\x9206, pages 723\\x96730, Sydney,\\nAustralia, July. Association for Computational\\nLinguistics.\\nHolger Schwenk, Anthony Rousseau, and Mohammed\\nAttik. 2012. Large, pruned or continuous space\\nlanguage models on a gpu for statistical machine\\ntranslation. In Proceedings of the NAACL-HLT 2012\\nWorkshop: Will We Ever Really Replace the N-gram\\nModel? On the Future of LanguageModeling for HLT,\\nWLM \\x9212, pages 11\\x9619, Montreal, Canada, June.\\nAssociation for Computational Linguistics.\\nHolger Schwenk. 2007. Continuous space language\\nmodels. Computer Speech and Language, 21(3):492\\x96\\n518.\\nHolger Schwenk. 2010. Continuous-space language\\nmodels for statistical machine translation. The Prague\\nBulletin of Mathematical Linguistics, pages 137\\x96146.\\nLe Hai Son, Alexandre Allauzen, Guillaume Wisniewski,\\nand Franc¸ois Yvon. 2010. Training continuous\\nspace language models: some practical issues. In\\nProceedings of the 2010 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP\\n\\x9210, pages 778\\x96788, Cambridge, Massachusetts,\\nOctober. Association for Computational Linguistics.\\nLe Hai Son, Alexandre Allauzen, and Franc¸ois Yvon.\\n2012. Continuous space translation models with\\nneural networks. In Proceedings of the 2012\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, NAACL HLT \\x9212, pages\\n39\\x9648, Montreal, Canada, June. Association for\\nComputational Linguistics.\\nAndreas Stolcke. 2002. Srilm-an extensible language\\nmodeling toolkit. In Proceedings International\\nConference on Spoken Language Processing, pages\\n257\\x96286, November.\\nOmar F. Zaidan. 2009. Z-MERT: A fully configurable\\nopen source tool for minimum error rate training of\\nmachine translation systems. The Prague Bulletin of\\nMathematical Linguistics, 91:79\\x9688.\\n850\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}